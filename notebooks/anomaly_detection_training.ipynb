{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "title_header"
   },
   "source": [
    "# Snowcore Anomaly Detection - Multi-Model Training\n",
    "\n",
    "## Business Objective\n",
    "\n",
    "Snowcore Industries faces intermittent quality yield issues on the Avalanche X1 bobsled production line. This notebook trains three complementary anomaly detection models:\n",
    "\n",
    "| Model | Asset Focus | Method | Output |\n",
    "|-------|-------------|--------|--------|\n",
    "| **Model A** | Layup Room | Linear Regression | Scrap risk from humidity |\n",
    "| **Model B** | Autoclave | PCA Autoencoder | Cure cycle anomaly score |\n",
    "| **Model C** | Cross-Asset | Graph Analysis | Downstream impact propagation |\n",
    "\n",
    "## Hidden Discovery\n",
    "\n",
    "The key insight: **High humidity (>65%) in Layup Room causes 3x scrap rate 6 hours later during autoclave cure** due to moisture-induced delamination.\n",
    "\n",
    "## Output Tables\n",
    "\n",
    "- `PDM.ANOMALY_EVENTS` - Detected anomalies with severity and root cause\n",
    "- `PDM.MODEL_METRICS` - Model performance metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "setup_header"
   },
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "imports_cell"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams.update({\n",
    "    'figure.facecolor': '#121212',\n",
    "    'axes.facecolor': '#121212',\n",
    "    'text.color': '#E5E5E7',\n",
    "    'figure.dpi': 150,\n",
    "    'figure.figsize': (12, 5)\n",
    "})\n",
    "\n",
    "COLORS = {\n",
    "    'primary': '#29B5E8',\n",
    "    'secondary': '#11567F',\n",
    "    'accent': '#FF9F0A',\n",
    "    'warning': '#ef4444',\n",
    "    'success': '#22c55e'\n",
    "}\n",
    "\n",
    "np.random.seed(42)\n",
    "print('[OK] Libraries imported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "session_cell"
   },
   "outputs": [],
   "source": [
    "from snowflake.snowpark.context import get_active_session\n",
    "\n",
    "session = get_active_session()\n",
    "\n",
    "print(f'[OK] Connected to Snowflake')\n",
    "print(f'  Database: {session.get_current_database()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "data_loading_header"
   },
   "source": [
    "## 2. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "load_snowflake_cell"
   },
   "outputs": [],
   "source": [
    "print('Loading data from Snowflake...')\n",
    "\n",
    "cure_results_df = session.table('PDM.CURE_RESULTS').to_pandas()\n",
    "print(f'  PDM.CURE_RESULTS: {len(cure_results_df):,} rows')\n",
    "\n",
    "asset_graph_df = session.table('CONFIG.ASSET_GRAPH').to_pandas()\n",
    "print(f'  CONFIG.ASSET_GRAPH: {len(asset_graph_df):,} rows')\n",
    "\n",
    "asset_status_df = session.table('DATA_MART.ASSET_STATUS').to_pandas()\n",
    "print(f'  DATA_MART.ASSET_STATUS: {len(asset_status_df):,} rows')\n",
    "\n",
    "try:\n",
    "    sensors_wide_df = session.sql(\"\"\"\n",
    "        SELECT * FROM ATOMIC.ASSET_SENSORS_WIDE \n",
    "        WHERE EVENT_TIMESTAMP > DATEADD('day', -30, CURRENT_TIMESTAMP())\n",
    "        LIMIT 10000\n",
    "    \"\"\").to_pandas()\n",
    "    print(f'  ATOMIC.ASSET_SENSORS_WIDE: {len(sensors_wide_df):,} rows')\n",
    "except:\n",
    "    sensors_wide_df = pd.DataFrame()\n",
    "    print('  ATOMIC.ASSET_SENSORS_WIDE: No data (will use synthetic)')\n",
    "\n",
    "print('\\n[OK] Data loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "synthetic_fallback_cell"
   },
   "outputs": [],
   "source": [
    "if cure_results_df.empty:\n",
    "    print('Generating synthetic cure results with humidity-scrap correlation...')\n",
    "    \n",
    "    n_batches = 500\n",
    "    cure_results_df = pd.DataFrame({\n",
    "        'RESULT_ID': [f'CR-{i:04d}' for i in range(n_batches)],\n",
    "        'BATCH_ID': [f'BATCH-{i:04d}' for i in range(n_batches)],\n",
    "        'AUTOCLAVE_ID': np.random.choice(['AUTOCLAVE_01', 'AUTOCLAVE_02'], n_batches),\n",
    "        'CURE_TIMESTAMP': pd.date_range(end=datetime.now(), periods=n_batches, freq='2H'),\n",
    "        'LAYUP_HUMIDITY_AVG': np.random.uniform(45, 75, n_batches),\n",
    "        'LAYUP_HUMIDITY_PEAK': np.random.uniform(50, 85, n_batches),\n",
    "    })\n",
    "    \n",
    "    cure_results_df['SCRAP_FLAG'] = (\n",
    "        (cure_results_df['LAYUP_HUMIDITY_PEAK'] > 65) & \n",
    "        (np.random.random(n_batches) < 0.48)\n",
    "    ) | (\n",
    "        (cure_results_df['LAYUP_HUMIDITY_PEAK'] <= 65) & \n",
    "        (np.random.random(n_batches) < 0.05)\n",
    "    )\n",
    "    \n",
    "    cure_results_df['DELAMINATION_SCORE'] = np.where(\n",
    "        cure_results_df['SCRAP_FLAG'],\n",
    "        np.random.uniform(0.6, 1.0, n_batches),\n",
    "        np.random.uniform(0.0, 0.3, n_batches)\n",
    "    )\n",
    "    \n",
    "    print(f'  Generated {n_batches} synthetic batches')\n",
    "    print(f'  Scrap rate: {cure_results_df[\"SCRAP_FLAG\"].mean():.1%}')\n",
    "\n",
    "if sensors_wide_df.empty:\n",
    "    print('Generating synthetic sensor data...')\n",
    "    \n",
    "    assets = ['AUTOCLAVE_01', 'AUTOCLAVE_02', 'LAYUP_ROOM', 'CNC_MILL_01', 'CNC_MILL_02']\n",
    "    timestamps = pd.date_range(end=datetime.now(), periods=1000, freq='1min')\n",
    "    \n",
    "    rows = []\n",
    "    for ts in timestamps:\n",
    "        for asset in assets:\n",
    "            row = {\n",
    "                'EVENT_TIMESTAMP': ts,\n",
    "                'ASSET_ID': asset,\n",
    "                'TEMPERATURE_C': np.random.uniform(150, 200) if 'AUTOCLAVE' in asset else np.random.uniform(20, 25),\n",
    "                'PRESSURE_PSI': np.random.uniform(80, 120) if 'AUTOCLAVE' in asset else None,\n",
    "                'VACUUM_MBAR': np.random.uniform(-1.0, -0.85) if 'AUTOCLAVE' in asset else None,\n",
    "                'HUMIDITY_PCT': np.random.uniform(45, 72) if asset == 'LAYUP_ROOM' else None,\n",
    "                'VIBRATION_G': np.random.uniform(0.1, 0.6) if 'CNC' in asset else None,\n",
    "            }\n",
    "            rows.append(row)\n",
    "    \n",
    "    sensors_wide_df = pd.DataFrame(rows)\n",
    "    print(f'  Generated {len(sensors_wide_df):,} sensor readings')\n",
    "\n",
    "print('\\n[OK] Data ready for training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "exploration_header"
   },
   "source": [
    "## 3. Data Exploration - Humidity-Scrap Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "correlation_analysis_cell"
   },
   "outputs": [],
   "source": [
    "print('=' * 60)\n",
    "print('HUMIDITY-SCRAP CORRELATION ANALYSIS')\n",
    "print('=' * 60)\n",
    "\n",
    "cure_results_df['HUMIDITY_CLASS'] = np.where(\n",
    "    cure_results_df['LAYUP_HUMIDITY_PEAK'] > 65, 'HIGH', 'NORMAL'\n",
    ")\n",
    "\n",
    "correlation = cure_results_df.groupby('HUMIDITY_CLASS').agg({\n",
    "    'BATCH_ID': 'count',\n",
    "    'SCRAP_FLAG': ['sum', 'mean'],\n",
    "    'DELAMINATION_SCORE': 'mean'\n",
    "}).round(3)\n",
    "\n",
    "correlation.columns = ['Batches', 'Scrapped', 'Scrap_Rate', 'Avg_Delamination']\n",
    "print('\\nScrap Rate by Humidity Class:')\n",
    "print(correlation.to_string())\n",
    "\n",
    "high_humidity_rate = cure_results_df[cure_results_df['HUMIDITY_CLASS'] == 'HIGH']['SCRAP_FLAG'].mean()\n",
    "normal_humidity_rate = cure_results_df[cure_results_df['HUMIDITY_CLASS'] == 'NORMAL']['SCRAP_FLAG'].mean()\n",
    "multiplier = high_humidity_rate / normal_humidity_rate if normal_humidity_rate > 0 else 0\n",
    "\n",
    "print(f'\\n[KEY FINDING] High humidity batches have {multiplier:.1f}x higher scrap rate!')\n",
    "print(f'  HIGH humidity (>65%): {high_humidity_rate:.1%} scrap rate')\n",
    "print(f'  NORMAL humidity: {normal_humidity_rate:.1%} scrap rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "model_a_header"
   },
   "source": [
    "## 4. Model A: Linear Regression (Layup Humidity)\n",
    "\n",
    "Predict delamination score from humidity features to identify at-risk batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "train_model_a_cell"
   },
   "outputs": [],
   "source": [
    "print('=' * 60)\n",
    "print('MODEL A: LAYUP HUMIDITY REGRESSION')\n",
    "print('=' * 60)\n",
    "\n",
    "feature_cols = ['LAYUP_HUMIDITY_AVG', 'LAYUP_HUMIDITY_PEAK']\n",
    "X = cure_results_df[feature_cols].values\n",
    "y = cure_results_df['DELAMINATION_SCORE'].values\n",
    "\n",
    "split_idx = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "model_layup = LinearRegression()\n",
    "model_layup.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model_layup.predict(X_test)\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(f'\\nModel Performance:')\n",
    "print(f'  R2 Score: {r2:.4f}')\n",
    "print(f'  MAE: {mae:.4f}')\n",
    "\n",
    "print(f'\\nCoefficients:')\n",
    "for col, coef in zip(feature_cols, model_layup.coef_):\n",
    "    print(f'  {col}: {coef:.4f}')\n",
    "print(f'  Intercept: {model_layup.intercept_:.4f}')\n",
    "\n",
    "model_a_metrics = {\n",
    "    'model_name': 'LAYUP_HUMIDITY_MODEL',\n",
    "    'model_type': 'LinearRegression',\n",
    "    'r2_score': float(r2),\n",
    "    'mae': float(mae),\n",
    "    'coefficients': dict(zip(feature_cols, [float(c) for c in model_layup.coef_])),\n",
    "    'intercept': float(model_layup.intercept_)\n",
    "}\n",
    "\n",
    "print('\\n[OK] Model A trained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "inference_model_a_cell"
   },
   "outputs": [],
   "source": [
    "def predict_scrap_risk(humidity_avg, humidity_peak):\n",
    "    \"\"\"Predict delamination risk from humidity readings.\"\"\"\n",
    "    predicted_score = model_layup.predict([[humidity_avg, humidity_peak]])[0]\n",
    "    \n",
    "    if predicted_score > 0.6:\n",
    "        severity = 'CRITICAL'\n",
    "    elif predicted_score > 0.4:\n",
    "        severity = 'WARNING'\n",
    "    else:\n",
    "        severity = 'NORMAL'\n",
    "    \n",
    "    return {\n",
    "        'predicted_delamination': round(predicted_score, 3),\n",
    "        'severity': severity,\n",
    "        'root_cause': 'High ambient humidity during layup' if humidity_peak > 65 else 'Normal humidity levels',\n",
    "        'suggested_fix': 'Activate dehumidifiers, delay layup operations' if humidity_peak > 65 else 'Continue normal operations'\n",
    "    }\n",
    "\n",
    "print('Testing Model A inference:')\n",
    "test_cases = [(50, 55), (62, 68), (70, 78)]\n",
    "\n",
    "for avg, peak in test_cases:\n",
    "    result = predict_scrap_risk(avg, peak)\n",
    "    print(f'\\n  Humidity: {avg}/{peak}% -> {result[\"severity\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "model_b_header"
   },
   "source": [
    "## 5. Model B: PCA Autoencoder (Autoclave Anomaly)\n",
    "\n",
    "Detect anomalies in autoclave cure cycles by measuring reconstruction error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "train_model_b_cell"
   },
   "outputs": [],
   "source": [
    "print('=' * 60)\n",
    "print('MODEL B: AUTOCLAVE ANOMALY DETECTION')\n",
    "print('=' * 60)\n",
    "\n",
    "autoclave_features = ['TEMPERATURE_C', 'PRESSURE_PSI', 'VACUUM_MBAR']\n",
    "\n",
    "autoclave_data = sensors_wide_df[sensors_wide_df['ASSET_ID'].str.contains('AUTOCLAVE', na=False)].copy()\n",
    "autoclave_data = autoclave_data.dropna(subset=autoclave_features)\n",
    "\n",
    "print(f'Autoclave readings: {len(autoclave_data):,}')\n",
    "\n",
    "scaler_autoclave = StandardScaler()\n",
    "X_scaled = scaler_autoclave.fit_transform(autoclave_data[autoclave_features])\n",
    "\n",
    "pca_autoclave = PCA(n_components=2)\n",
    "X_compressed = pca_autoclave.fit_transform(X_scaled)\n",
    "\n",
    "X_reconstructed = pca_autoclave.inverse_transform(X_compressed)\n",
    "reconstruction_errors = np.mean((X_scaled - X_reconstructed)**2, axis=1)\n",
    "\n",
    "threshold_autoclave = np.percentile(reconstruction_errors, 95)\n",
    "print(f'Anomaly threshold (95th percentile): {threshold_autoclave:.4f}')\n",
    "\n",
    "anomaly_rate = (reconstruction_errors > threshold_autoclave).mean()\n",
    "print(f'Training anomaly rate: {anomaly_rate:.1%}')\n",
    "\n",
    "print(f'\\nPCA explained variance: {pca_autoclave.explained_variance_ratio_.sum():.1%}')\n",
    "\n",
    "model_b_metrics = {\n",
    "    'model_name': 'AUTOCLAVE_ANOMALY_MODEL',\n",
    "    'model_type': 'PCA_Autoencoder',\n",
    "    'n_components': 2,\n",
    "    'explained_variance': float(pca_autoclave.explained_variance_ratio_.sum()),\n",
    "    'threshold': float(threshold_autoclave),\n",
    "    'training_anomaly_rate': float(anomaly_rate)\n",
    "}\n",
    "\n",
    "print('\\n[OK] Model B trained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "inference_model_b_cell"
   },
   "outputs": [],
   "source": [
    "def detect_autoclave_anomaly(temperature, pressure, vacuum):\n",
    "    \"\"\"Detect autoclave anomaly using PCA reconstruction error.\"\"\"\n",
    "    X = np.array([[temperature, pressure, vacuum]])\n",
    "    X_scaled = scaler_autoclave.transform(X)\n",
    "    X_compressed = pca_autoclave.transform(X_scaled)\n",
    "    X_reconstructed = pca_autoclave.inverse_transform(X_compressed)\n",
    "    error = np.mean((X_scaled - X_reconstructed)**2)\n",
    "    \n",
    "    is_anomaly = error > threshold_autoclave\n",
    "    anomaly_score = min(1.0, error / (threshold_autoclave * 2))\n",
    "    \n",
    "    anomaly_type = None\n",
    "    root_cause = 'Normal operation'\n",
    "    suggested_fix = 'Continue monitoring'\n",
    "    \n",
    "    if vacuum > -0.9:\n",
    "        anomaly_type = 'VACUUM_DEGRADATION'\n",
    "        root_cause = 'Vacuum seal wear or leak detected'\n",
    "        suggested_fix = 'Inspect door seal and gaskets, check vacuum pump'\n",
    "    elif temperature > 195 or temperature < 155:\n",
    "        anomaly_type = 'TEMPERATURE_DEVIATION'\n",
    "        root_cause = 'Temperature outside optimal cure range'\n",
    "        suggested_fix = 'Check heating elements and thermocouples'\n",
    "    elif pressure > 115 or pressure < 85:\n",
    "        anomaly_type = 'PRESSURE_DEVIATION'\n",
    "        root_cause = 'Pressure regulation issue'\n",
    "        suggested_fix = 'Inspect pressure relief valves and regulators'\n",
    "    elif is_anomaly:\n",
    "        anomaly_type = 'PATTERN_ANOMALY'\n",
    "        root_cause = 'Unusual sensor pattern detected'\n",
    "        suggested_fix = 'Review cure cycle parameters'\n",
    "    \n",
    "    return {\n",
    "        'is_anomaly': is_anomaly,\n",
    "        'anomaly_score': round(anomaly_score, 3),\n",
    "        'anomaly_type': anomaly_type,\n",
    "        'severity': 'CRITICAL' if anomaly_score > 0.7 else 'WARNING' if anomaly_score > 0.4 else 'INFO',\n",
    "        'root_cause': root_cause,\n",
    "        'suggested_fix': suggested_fix\n",
    "    }\n",
    "\n",
    "print('Testing Model B inference:')\n",
    "test_cases = [(175, 100, -0.95), (180, 105, -0.88), (200, 120, -0.75)]\n",
    "\n",
    "for temp, pressure, vacuum in test_cases:\n",
    "    result = detect_autoclave_anomaly(temp, pressure, vacuum)\n",
    "    status = 'ANOMALY' if result['is_anomaly'] else 'NORMAL'\n",
    "    print(f'\\n  T={temp}C, P={pressure}psi, V={vacuum}mbar -> {status}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "model_c_header"
   },
   "source": [
    "## 6. Model C: Graph Propagation (Cross-Asset)\n",
    "\n",
    "Propagate anomaly signals through the asset dependency graph to predict downstream impacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "build_graph_cell"
   },
   "outputs": [],
   "source": [
    "print('=' * 60)\n",
    "print('MODEL C: CROSS-ASSET GRAPH PROPAGATION')\n",
    "print('=' * 60)\n",
    "\n",
    "G = nx.DiGraph()\n",
    "\n",
    "if not asset_graph_df.empty:\n",
    "    for _, row in asset_graph_df.iterrows():\n",
    "        G.add_edge(\n",
    "            row['SOURCE_ASSET'], \n",
    "            row['TARGET_ASSET'],\n",
    "            edge_type=row['EDGE_TYPE'],\n",
    "            weight=row['WEIGHT'],\n",
    "            lag_hours=row['LAG_HOURS']\n",
    "        )\n",
    "else:\n",
    "    edges = [\n",
    "        ('LAYUP_ROOM', 'LAYUP_BOT_01', 'ENV_INFLUENCE', 0.8, 0),\n",
    "        ('LAYUP_ROOM', 'LAYUP_BOT_02', 'ENV_INFLUENCE', 0.8, 0),\n",
    "        ('LAYUP_BOT_01', 'AUTOCLAVE_01', 'MATERIAL_FLOW', 1.0, 2),\n",
    "        ('LAYUP_BOT_02', 'AUTOCLAVE_02', 'MATERIAL_FLOW', 1.0, 2),\n",
    "        ('AUTOCLAVE_01', 'CNC_MILL_01', 'MATERIAL_FLOW', 1.0, 4),\n",
    "        ('AUTOCLAVE_02', 'CNC_MILL_02', 'MATERIAL_FLOW', 1.0, 4),\n",
    "        ('CNC_MILL_01', 'QC_STATION_01', 'MATERIAL_FLOW', 1.0, 1),\n",
    "        ('CNC_MILL_02', 'QC_STATION_02', 'MATERIAL_FLOW', 1.0, 1),\n",
    "    ]\n",
    "    for src, tgt, etype, weight, lag in edges:\n",
    "        G.add_edge(src, tgt, edge_type=etype, weight=weight, lag_hours=lag)\n",
    "\n",
    "print(f'Graph nodes: {G.number_of_nodes()}')\n",
    "print(f'Graph edges: {G.number_of_edges()}')\n",
    "print(f'\\nAssets: {list(G.nodes())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "propagation_func_cell"
   },
   "outputs": [],
   "source": [
    "def propagate_anomaly(source_asset, anomaly_score, anomaly_type):\n",
    "    \"\"\"Propagate anomaly signal through asset dependency graph.\"\"\"\n",
    "    if source_asset not in G:\n",
    "        return []\n",
    "    \n",
    "    affected_assets = []\n",
    "    \n",
    "    for target in nx.descendants(G, source_asset):\n",
    "        try:\n",
    "            path = nx.shortest_path(G, source_asset, target)\n",
    "        except nx.NetworkXNoPath:\n",
    "            continue\n",
    "        \n",
    "        propagated_score = anomaly_score\n",
    "        total_lag = 0\n",
    "        \n",
    "        for i in range(len(path) - 1):\n",
    "            edge = G[path[i]][path[i+1]]\n",
    "            propagated_score *= edge['weight']\n",
    "            total_lag += edge['lag_hours']\n",
    "        \n",
    "        if propagated_score > 0.3:\n",
    "            affected_assets.append({\n",
    "                'target_asset': target,\n",
    "                'propagated_score': round(propagated_score, 3),\n",
    "                'lag_hours': total_lag,\n",
    "                'path': ' -> '.join(path),\n",
    "                'risk_level': 'HIGH' if propagated_score > 0.7 else 'MEDIUM' if propagated_score > 0.4 else 'LOW',\n",
    "                'anomaly_type': f'PROPAGATED_{anomaly_type}',\n",
    "                'root_cause': f'Upstream anomaly from {source_asset}',\n",
    "                'expected_impact_time': f'+{total_lag}h'\n",
    "            })\n",
    "    \n",
    "    return sorted(affected_assets, key=lambda x: x['propagated_score'], reverse=True)\n",
    "\n",
    "print('Testing Model C propagation (HIGH_HUMIDITY in LAYUP_ROOM):')\n",
    "propagation = propagate_anomaly('LAYUP_ROOM', 0.85, 'HIGH_HUMIDITY')\n",
    "\n",
    "for asset in propagation:\n",
    "    print(f'\\n  {asset[\"target_asset\"]}:')\n",
    "    print(f'    Risk: {asset[\"risk_level\"]} ({asset[\"propagated_score\"]})')\n",
    "    print(f'    Expected impact: {asset[\"expected_impact_time\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "combined_header"
   },
   "source": [
    "## 7. Combined Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "combined_inference_cell"
   },
   "outputs": [],
   "source": [
    "def detect_all_anomalies(asset_id, sensor_data):\n",
    "    \"\"\"Run all three models to detect anomalies for a given asset.\"\"\"\n",
    "    anomalies = []\n",
    "    \n",
    "    if 'LAYUP_ROOM' in asset_id:\n",
    "        humidity_avg = sensor_data.get('HUMIDITY_AVG', 50)\n",
    "        humidity_peak = sensor_data.get('HUMIDITY_PEAK', sensor_data.get('HUMIDITY_PCT', 50))\n",
    "        \n",
    "        result = predict_scrap_risk(humidity_avg, humidity_peak)\n",
    "        \n",
    "        if result['severity'] != 'NORMAL':\n",
    "            anomaly_score = min(1.0, (humidity_peak - 65) / 20) if humidity_peak > 65 else 0.3\n",
    "            anomalies.append({\n",
    "                'asset_id': asset_id,\n",
    "                'anomaly_type': 'HIGH_HUMIDITY',\n",
    "                'anomaly_score': anomaly_score,\n",
    "                'severity': result['severity'],\n",
    "                'root_cause': result['root_cause'],\n",
    "                'suggested_fix': result['suggested_fix'],\n",
    "                'model': 'LAYUP_HUMIDITY_MODEL'\n",
    "            })\n",
    "            \n",
    "            propagated = propagate_anomaly(asset_id, anomaly_score, 'HIGH_HUMIDITY')\n",
    "            for prop in propagated:\n",
    "                anomalies.append({\n",
    "                    'asset_id': prop['target_asset'],\n",
    "                    'anomaly_type': prop['anomaly_type'],\n",
    "                    'anomaly_score': prop['propagated_score'],\n",
    "                    'severity': 'WARNING' if prop['propagated_score'] > 0.5 else 'INFO',\n",
    "                    'root_cause': prop['root_cause'],\n",
    "                    'suggested_fix': f'Monitor for impact in {prop[\"expected_impact_time\"]}',\n",
    "                    'model': 'GRAPH_PROPAGATION'\n",
    "                })\n",
    "    \n",
    "    elif 'AUTOCLAVE' in asset_id:\n",
    "        temp = sensor_data.get('TEMPERATURE_C', 175)\n",
    "        pressure = sensor_data.get('PRESSURE_PSI', 100)\n",
    "        vacuum = sensor_data.get('VACUUM_MBAR', -0.95)\n",
    "        \n",
    "        result = detect_autoclave_anomaly(temp, pressure, vacuum)\n",
    "        \n",
    "        if result['is_anomaly'] or result['anomaly_type']:\n",
    "            anomalies.append({\n",
    "                'asset_id': asset_id,\n",
    "                'anomaly_type': result['anomaly_type'] or 'CURE_ANOMALY',\n",
    "                'anomaly_score': result['anomaly_score'],\n",
    "                'severity': result['severity'],\n",
    "                'root_cause': result['root_cause'],\n",
    "                'suggested_fix': result['suggested_fix'],\n",
    "                'model': 'AUTOCLAVE_ANOMALY_MODEL'\n",
    "            })\n",
    "    \n",
    "    elif 'CNC_MILL' in asset_id:\n",
    "        vibration = sensor_data.get('VIBRATION_G', 0.3)\n",
    "        \n",
    "        if vibration > 0.8:\n",
    "            anomalies.append({\n",
    "                'asset_id': asset_id,\n",
    "                'anomaly_type': 'VIBRATION_SPIKE',\n",
    "                'anomaly_score': min(1.0, vibration / 1.2),\n",
    "                'severity': 'CRITICAL' if vibration > 1.0 else 'WARNING',\n",
    "                'root_cause': 'Spindle bearing wear or imbalance',\n",
    "                'suggested_fix': 'Check spindle alignment, inspect bearings',\n",
    "                'model': 'THRESHOLD_DETECTION'\n",
    "            })\n",
    "    \n",
    "    return anomalies\n",
    "\n",
    "print('=' * 60)\n",
    "print('COMBINED INFERENCE TEST')\n",
    "print('=' * 60)\n",
    "\n",
    "test_scenarios = [\n",
    "    ('LAYUP_ROOM', {'HUMIDITY_AVG': 68, 'HUMIDITY_PEAK': 75}),\n",
    "    ('AUTOCLAVE_01', {'TEMPERATURE_C': 180, 'PRESSURE_PSI': 105, 'VACUUM_MBAR': -0.82}),\n",
    "    ('CNC_MILL_01', {'VIBRATION_G': 0.95}),\n",
    "]\n",
    "\n",
    "for asset_id, sensor_data in test_scenarios:\n",
    "    print(f'\\n{asset_id}:')\n",
    "    anomalies = detect_all_anomalies(asset_id, sensor_data)\n",
    "    if anomalies:\n",
    "        for a in anomalies:\n",
    "            print(f'  [{a[\"severity\"]}] {a[\"anomaly_type\"]}: {a[\"anomaly_score\"]:.2f}')\n",
    "    else:\n",
    "        print('  [OK] No anomalies detected')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "output_header"
   },
   "source": [
    "## 8. Write Results to Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "write_metrics_cell"
   },
   "outputs": [],
   "source": [
    "print('Writing model metrics to Snowflake...')\n",
    "\n",
    "session.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS PDM.MODEL_METRICS (\n",
    "    METRIC_ID STRING DEFAULT UUID_STRING(),\n",
    "    MODEL_NAME STRING,\n",
    "    MODEL_TYPE STRING,\n",
    "    METRICS VARIANT,\n",
    "    TRAINED_AT TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP()\n",
    ")\n",
    "\"\"\").collect()\n",
    "\n",
    "metrics_df = pd.DataFrame([\n",
    "    {\n",
    "        'MODEL_NAME': model_a_metrics['model_name'],\n",
    "        'MODEL_TYPE': model_a_metrics['model_type'],\n",
    "        'METRICS': json.dumps(model_a_metrics)\n",
    "    },\n",
    "    {\n",
    "        'MODEL_NAME': model_b_metrics['model_name'],\n",
    "        'MODEL_TYPE': model_b_metrics['model_type'],\n",
    "        'METRICS': json.dumps(model_b_metrics)\n",
    "    },\n",
    "    {\n",
    "        'MODEL_NAME': 'GRAPH_PROPAGATION_MODEL',\n",
    "        'MODEL_TYPE': 'NetworkX_DiGraph',\n",
    "        'METRICS': json.dumps({\n",
    "            'nodes': G.number_of_nodes(),\n",
    "            'edges': G.number_of_edges(),\n",
    "            'assets': list(G.nodes())\n",
    "        })\n",
    "    }\n",
    "])\n",
    "\n",
    "session.write_pandas(metrics_df, 'MODEL_METRICS', database='SNOWCORE_PDM', schema='PDM', overwrite=False)\n",
    "print(f'  PDM.MODEL_METRICS: {len(metrics_df)} rows written')\n",
    "print('\\n[OK] Model metrics saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67da0bc6",
   "metadata": {
    "name": "write_diagnostics_cell"
   },
   "outputs": [],
   "source": [
    "print('Writing model diagnostics to Snowflake...')\n",
    "\n",
    "diagnostics = []\n",
    "\n",
    "diagnostics.append({\n",
    "    'MODEL_NAME': model_a_metrics['model_name'],\n",
    "    'MODEL_VERSION': '1.0',\n",
    "    'METRIC_NAME': 'r2_score',\n",
    "    'METRIC_VALUE': model_a_metrics['r2_score'],\n",
    "    'THRESHOLD_VALUE': 0.7,\n",
    "    'STATUS': 'PASS' if model_a_metrics['r2_score'] >= 0.7 else 'FAIL',\n",
    "    'DETAILS': json.dumps({'rmse': model_a_metrics['rmse'], 'mae': model_a_metrics['mae']})\n",
    "})\n",
    "\n",
    "diagnostics.append({\n",
    "    'MODEL_NAME': model_b_metrics['model_name'],\n",
    "    'MODEL_VERSION': '1.0',\n",
    "    'METRIC_NAME': 'reconstruction_error_threshold',\n",
    "    'METRIC_VALUE': model_b_metrics['threshold'],\n",
    "    'THRESHOLD_VALUE': 0.5,\n",
    "    'STATUS': 'PASS' if model_b_metrics['threshold'] <= 0.5 else 'WARN',\n",
    "    'DETAILS': json.dumps({'variance_explained': model_b_metrics['variance_explained'], 'n_components': model_b_metrics['n_components']})\n",
    "})\n",
    "\n",
    "diagnostics.append({\n",
    "    'MODEL_NAME': 'GRAPH_PROPAGATION_MODEL',\n",
    "    'MODEL_VERSION': '1.0',\n",
    "    'METRIC_NAME': 'graph_connectivity',\n",
    "    'METRIC_VALUE': G.number_of_edges() / max(G.number_of_nodes(), 1),\n",
    "    'THRESHOLD_VALUE': 0.5,\n",
    "    'STATUS': 'PASS' if G.number_of_edges() >= G.number_of_nodes() * 0.5 else 'WARN',\n",
    "    'DETAILS': json.dumps({'nodes': G.number_of_nodes(), 'edges': G.number_of_edges()})\n",
    "})\n",
    "\n",
    "diagnostics_df = pd.DataFrame(diagnostics)\n",
    "session.write_pandas(diagnostics_df, 'MODEL_DIAGNOSTICS', database='SNOWCORE_PDM', schema='PDM', overwrite=False)\n",
    "print(f'  PDM.MODEL_DIAGNOSTICS: {len(diagnostics_df)} rows written')\n",
    "print('\\n[OK] Model diagnostics saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "write_anomalies_cell"
   },
   "outputs": [],
   "source": [
    "print('Writing sample anomaly events to Snowflake...')\n",
    "\n",
    "sample_anomalies = []\n",
    "\n",
    "for asset_id, sensor_data in test_scenarios:\n",
    "    detected = detect_all_anomalies(asset_id, sensor_data)\n",
    "    for a in detected:\n",
    "        sample_anomalies.append({\n",
    "            'ASSET_ID': a['asset_id'],\n",
    "            'TIMESTAMP': datetime.now(),\n",
    "            'ANOMALY_TYPE': a['anomaly_type'],\n",
    "            'ANOMALY_SCORE': a['anomaly_score'],\n",
    "            'SEVERITY': a['severity'],\n",
    "            'ROOT_CAUSE': a['root_cause'],\n",
    "            'SUGGESTED_FIX': a['suggested_fix'],\n",
    "            'RESOLVED': False\n",
    "        })\n",
    "\n",
    "if sample_anomalies:\n",
    "    anomalies_df = pd.DataFrame(sample_anomalies)\n",
    "    session.write_pandas(anomalies_df, 'ANOMALY_EVENTS', database='SNOWCORE_PDM', schema='PDM', overwrite=False)\n",
    "    print(f'  PDM.ANOMALY_EVENTS: {len(anomalies_df)} rows written')\n",
    "\n",
    "print('\\n[OK] Sample anomaly events saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "summary_header"
   },
   "source": [
    "## 9. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "summary_cell"
   },
   "outputs": [],
   "source": [
    "print('=' * 60)\n",
    "print('ANOMALY DETECTION TRAINING - COMPLETE')\n",
    "print('=' * 60)\n",
    "\n",
    "print('\\nModels Trained:')\n",
    "print(f'  1. LAYUP_HUMIDITY_MODEL (LinearRegression)')\n",
    "print(f'     R2: {model_a_metrics[\"r2_score\"]:.4f}, MAE: {model_a_metrics[\"mae\"]:.4f}')\n",
    "print(f'  2. AUTOCLAVE_ANOMALY_MODEL (PCA Autoencoder)')\n",
    "print(f'     Variance: {model_b_metrics[\"explained_variance\"]:.1%}, Threshold: {model_b_metrics[\"threshold\"]:.4f}')\n",
    "print(f'  3. GRAPH_PROPAGATION_MODEL (NetworkX)')\n",
    "print(f'     Nodes: {G.number_of_nodes()}, Edges: {G.number_of_edges()}')\n",
    "\n",
    "print('\\nKey Findings:')\n",
    "print(f'  - High humidity (>65%) causes {multiplier:.1f}x higher scrap rate')\n",
    "print(f'  - Anomalies propagate through graph with decay')\n",
    "print(f'  - Expected lag from LAYUP_ROOM to QC: 7 hours')\n",
    "\n",
    "print('\\nTables Updated:')\n",
    "print(f'  - PDM.MODEL_METRICS: Model performance tracking')\n",
    "print(f'  - PDM.ANOMALY_EVENTS: Sample anomaly detections')\n",
    "\n",
    "print('\\n[OK] Ready for inference deployment')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}